{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "NLP Sample"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "speech1 = 'Did you know your team can build PowerPoint muscles? Yes, I help build PowerPoint muscles. I teach people how to use PowerPoint more effectively in business. Now, for instance, I’m working with a global consulting firm to train all their senior consultants to give better sales presentations so they can close more business.'\n",
    "speech2 = 'I train people on how to make sure their PowerPoint slides aren’t a complete disaster. Those who attend my workshop can create slides that are 50 percent more clear and 50 percent more convincing by the end of the training, based on scores students give each other before and after the workshop. I’m not sure if my training could work at your company. But I’d be happy to talk to you about it.'\n",
    "speech3 = 'You know how most business people use PowerPoint but most use it pretty poorly? Well, bad PowerPoint has all kinds of consequences – sales that don’t close, good ideas that get ignored, time wasted building slides that could have been used developing or executing strategies. My company shows businesses how to use PowerPoint to capture those sales, bring attention to those great ideas, and use those wasted hours on more important projects.'\n",
    "\n",
    "speeches = [\n",
    "    word_tokenize(speech1),\n",
    "    word_tokenize(speech2),\n",
    "    word_tokenize(speech3)\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Remove Stop Words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = {'?', '’', ',', 'you', 'doing', 'and', 'after', 'there', 'so', \"wasn't\", 're', 'again', 'only', 'weren', 'yours', 'herself', 'into', \"shan't\", 'aren', 'not', 'ma', \"wouldn't\", 'of', 'the', 'now', 'between', 'shan', 'while', 'until', 'her', 'over', 'most', 'ours', 'some', \"shouldn't\", 'ain', 'mustn', 'did', 'an', 'she', 'under', \"didn't\", \"hadn't\", 'these', 'o', 'whom', 'will', 'ourselves', 'be', \"she's\", 'yourself', 'below', 'shouldn', 'how', 'then', 'didn', 'a', 'all', 'off', 'further', 'nor', 'hasn', 'before', 'out', 'which', 'against', 'any', 'what', 'couldn', 'this', 'why', 'doesn', 'he', \"isn't\", \"that'll\", 'just', \"haven't\", 'wasn', \"weren't\", 'because', 'through', 'down', 've', 'myself', 'they', 'above', \"you'll\", 'mightn', 'no',\n",
    "              'few', 'where', 'here', 'i', 'at', 'do', 'been', 'him', 'his', 'me', 'itself', 's', 'isn', 'in', 'by', 'we', \"won't\", 'their', 'on', \"couldn't\", 'wouldn', 'other', 'should', 'its', 'll', \"mightn't\", \"you've\", \"doesn't\", \"should've\", 'such', 'hadn', 'won', 'does', 'himself', \"needn't\", 't', 'each', 'm', 'my', 'themselves', 'was', 'once', \"don't\", 'or', 'don', \"you're\", 'about', 'can', 'too', 'those', 'is', 'who', 'when', 'theirs', 'if', \"it's\", 'd', 'yourselves', 'very', 'with', 'during', 'that', 'has', 'have', \"aren't\", 'were', 'own', 'more', 'for', 'needn', 'than', \"mustn't\", 'hers', 'but', \"you'd\", 'are', 'both', 'same', 'being', 'y', 'our', 'your', 'haven', 'had', 'as', \"hasn't\", 'it', 'am', 'to', 'having', 'from', 'up', 'them', '.', '-'}\n",
    "\n",
    "\n",
    "def filtered_text(doc):\n",
    "    filtered_sentence = []\n",
    "    for w in doc:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "filtered_speeches = []\n",
    "\n",
    "i = 0\n",
    "while i < len(speeches):\n",
    "    filtered_speeches.append(filtered_text(speeches[i]))\n",
    "    i += 1\n",
    "\n",
    "filtered_speeches"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Did',\n",
       "  'know',\n",
       "  'team',\n",
       "  'build',\n",
       "  'PowerPoint',\n",
       "  'muscles',\n",
       "  'Yes',\n",
       "  'I',\n",
       "  'help',\n",
       "  'build',\n",
       "  'PowerPoint',\n",
       "  'muscles',\n",
       "  'I',\n",
       "  'teach',\n",
       "  'people',\n",
       "  'use',\n",
       "  'PowerPoint',\n",
       "  'effectively',\n",
       "  'business',\n",
       "  'Now',\n",
       "  'instance',\n",
       "  'I',\n",
       "  'working',\n",
       "  'global',\n",
       "  'consulting',\n",
       "  'firm',\n",
       "  'train',\n",
       "  'senior',\n",
       "  'consultants',\n",
       "  'give',\n",
       "  'better',\n",
       "  'sales',\n",
       "  'presentations',\n",
       "  'close',\n",
       "  'business'],\n",
       " ['I',\n",
       "  'train',\n",
       "  'people',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'PowerPoint',\n",
       "  'slides',\n",
       "  'complete',\n",
       "  'disaster',\n",
       "  'Those',\n",
       "  'attend',\n",
       "  'workshop',\n",
       "  'create',\n",
       "  'slides',\n",
       "  '50',\n",
       "  'percent',\n",
       "  'clear',\n",
       "  '50',\n",
       "  'percent',\n",
       "  'convincing',\n",
       "  'end',\n",
       "  'training',\n",
       "  'based',\n",
       "  'scores',\n",
       "  'students',\n",
       "  'give',\n",
       "  'workshop',\n",
       "  'I',\n",
       "  'sure',\n",
       "  'training',\n",
       "  'could',\n",
       "  'work',\n",
       "  'company',\n",
       "  'But',\n",
       "  'I',\n",
       "  'happy',\n",
       "  'talk'],\n",
       " ['You',\n",
       "  'know',\n",
       "  'business',\n",
       "  'people',\n",
       "  'use',\n",
       "  'PowerPoint',\n",
       "  'use',\n",
       "  'pretty',\n",
       "  'poorly',\n",
       "  'Well',\n",
       "  'bad',\n",
       "  'PowerPoint',\n",
       "  'kinds',\n",
       "  'consequences',\n",
       "  '–',\n",
       "  'sales',\n",
       "  'close',\n",
       "  'good',\n",
       "  'ideas',\n",
       "  'get',\n",
       "  'ignored',\n",
       "  'time',\n",
       "  'wasted',\n",
       "  'building',\n",
       "  'slides',\n",
       "  'could',\n",
       "  'used',\n",
       "  'developing',\n",
       "  'executing',\n",
       "  'strategies',\n",
       "  'My',\n",
       "  'company',\n",
       "  'shows',\n",
       "  'businesses',\n",
       "  'use',\n",
       "  'PowerPoint',\n",
       "  'capture',\n",
       "  'sales',\n",
       "  'bring',\n",
       "  'attention',\n",
       "  'great',\n",
       "  'ideas',\n",
       "  'use',\n",
       "  'wasted',\n",
       "  'hours',\n",
       "  'important',\n",
       "  'projects']]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Vectorize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "speech_doc = []\n",
    "\n",
    "def covertToStringsFromTokenizedArray(textarr):\n",
    "    return \" \".join(textarr)\n",
    "\n",
    "j = 0\n",
    "while j < len(filtered_speeches):\n",
    "    speech_doc.append(covertToStringsFromTokenizedArray(filtered_speeches[j]))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_vec = vectorizer.fit_transform(speech_doc)\n",
    "print(x_vec.toarray())\n",
    "print(vectorizer.get_feature_names())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 0 0 0 0 1 0 2 0 2 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0\n",
      "  1 0 0 0 0 1 0 1 0 2 0 1 1 0 0 3 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0\n",
      "  0 0 0 1 0 1 0]\n",
      " [2 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 1 2 0 1 0 0 0 0 1 0 0 2 0 1 2 1 0 0 1 0 1 2 0 0\n",
      "  0 0 1 0 2 0 0]\n",
      " [0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0\n",
      "  0 1 2 1 1 0 1 1 0 0 1 0 1 0 1 3 0 1 1 2 0 0 1 1 1 0 0 0 0 0 0 1 0 0 4 1\n",
      "  2 1 0 0 0 0 1]]\n",
      "['50', 'attend', 'attention', 'bad', 'based', 'better', 'bring', 'build', 'building', 'business', 'businesses', 'but', 'capture', 'clear', 'close', 'company', 'complete', 'consequences', 'consultants', 'consulting', 'convincing', 'could', 'create', 'developing', 'did', 'disaster', 'effectively', 'end', 'executing', 'firm', 'get', 'give', 'global', 'good', 'great', 'happy', 'help', 'hours', 'ideas', 'ignored', 'important', 'instance', 'kinds', 'know', 'make', 'muscles', 'my', 'now', 'people', 'percent', 'poorly', 'powerpoint', 'presentations', 'pretty', 'projects', 'sales', 'scores', 'senior', 'shows', 'slides', 'strategies', 'students', 'sure', 'talk', 'teach', 'team', 'those', 'time', 'train', 'training', 'use', 'used', 'wasted', 'well', 'work', 'working', 'workshop', 'yes', 'you']\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "interpreter": {
   "hash": "8fe7413cb9d6ee0fbabcf6b75bee5d0ee49a9e0be0d8a8087d2a0637fe19406b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}